## 1장

### **1 텍스트 데이터의 특성과 전처리**

---

### **핵심 용어**

1. **순차 데이터 (Sequential Data)**
    - 순서에 의미가 있는 데이터.
    - 예시: 글, 대화, 일자별 날씨, 판매 실적 등.
2. **순환 신경망 (Recurrent Neural Network, RNN)**
    - 순차 데이터를 처리하기에 적합한 인공 신경망의 한 종류.
    - 순환층(Recurrent Layer)을 1개 이상 포함한 신경망.
3. **셀 (Cell)**
    - 순환층의 기본 구성 요소.
    - 여러 개의 뉴런으로 구성됨.
4. **은닉 상태 (Hidden State)**
    - 셀의 출력 값.
    - 다음 층으로 전달되며, 셀이 다음 타임스텝의 데이터를 처리할 때 재사용됨.

→ 엇 잘 모르겠다

- **은닉 상태의 추가 설명**
    
    은닉 상태가 다음 층으로 전달되고 재사용된다는 부분은 **RNN의 구조적 특징**과 밀접하게 관련되어 있습니다. 이를 이해하려면 은닉 상태의 역할과 순차 데이터 처리 방식에 대해 조금 더 자세히 살펴볼 필요가 있습니다.
    
    ---
    
    1. **은닉 상태란 무엇인가?**
        - RNN에서 각 타임스텝(time step)은 입력 데이터를 처리합니다.
        - 은닉 상태(Hidden State)는 **이전 타임스텝에서의 중요한 정보를 요약한 값**으로, 현재 타임스텝의 입력과 함께 다음 출력을 계산하는 데 사용됩니다.
        - 쉽게 말해, 은닉 상태는 **입력 데이터의 맥락(Context)을 기억하고 유지**하는 역할을 합니다.
    2. **다음 층으로 전달되는 이유**
        - RNN은 **순차 데이터의 시간적 의존성**을 처리하기 위해 설계되었습니다.
        - 은닉 상태를 다음 층으로 전달함으로써 **과거 정보가 현재와 미래의 계산에 반영**될 수 있습니다.
        - 예를 들어, 텍스트 데이터에서 "I like"라는 구문이 있다면, "I"의 정보는 "like"를 처리하는 데 필요합니다. 이 정보를 은닉 상태가 전달합니다.
    3. **타임스텝에서 재사용되는 이유**
        - 은닉 상태는 **각 타임스텝에서의 중간 결과물**이므로, 이후 타임스텝에서 새로운 입력 데이터를 처리할 때 **기존의 맥락과 결합**하여 업데이트됩니다.
        - 이 과정은 **순환(Recurrent)** 구조의 본질을 이루며, 데이터를 시간 축에 따라 단계적으로 처리하게 됩니다.
    4. **작동 방식 (수식으로 표현)**
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/e92bed44-5976-44f3-9a73-1b92d4d57f3f/e1ec3392-ea29-4892-9f8f-00a33a2599ae/image.png)
    
    ---
    
    바이어스에 대해서
    
    - 신경망에서 각 뉴런의 출력에 일정한 상수를 더해주는 값입니다.
    - 주어진 데이터 패턴에 맞춰 가중치와 함께 모델의 학습을 돕는 역할을 합니다.
    - 이를 통해 데이터의 **비선형성**을 더 잘 모델링할 수 있습니다.
    - 오차와는 다른 값, 오차를 줄이기 위해서 가중치와 함께 이용된다.

---

## 2장

### **2,3 RNN 구조의 구현과 학습 과정**

---

### **핵심 내용**

1. **RNN의 구조**
    - 입력층(Input Layer): 각 타임스텝에서 데이터의 특징을 벡터 형태로 전달.
    - 순환층(Recurrent Layer): 이전 은닉 상태와 현재 입력을 결합해 새로운 은닉 상태를 계산.
    - 출력층(Output Layer): 최종 은닉 상태를 기반으로 예측값 산출.
2. **학습 과정**
    
    RNN의 학습은 일반적인 신경망 학습과 비슷하지만, **시간적 의존성** 때문에 추가적인 고려가 필요합니다.
    
    - **순전파(Forward Propagation)**
        1. 각 타임스텝에서 입력 데이터를 은닉 상태로 전달.
        2. 출력층에서 최종 예측값을 계산.
    - **손실 함수(Loss Function)**
        - 예측값과 실제 값 간의 차이를 측정하는 함수.
        - 예: MSE(Mean Squared Error), Cross-Entropy Loss.
            
            MSE는 회귀 문제에서 주로 사용하는 손실 함수로, 예측값과 실제값 간의 차이를 제곱하여 평균을 취한 값입니다. 이는 모델의 예측이 실제값과 얼마나 가까운지를 측정하는 데 사용됩니다.
            
            Cross-Entropy Loss는 분류 문제에서 주로 사용하는 손실 함수로, 모델의 확률 분포가 실제 레이블과 얼마나 일치하는지를 측정합니다.
            
    - **역전파(Backpropagation Through Time, BPTT)**
        - 일반적인 역전파 알고리즘을 시간 축에 따라 확장한 방식.
        - **각 타임스텝의 기울기를 계산하고, 가중치를 업데이트**하여 학습.
3. **장기 의존성 문제**
    - RNN은 긴 시퀀스를 학습할 때 **기울기 소실(Vanishing Gradient)** 문제가 발생할 수 있습니다.
    - 이를 해결하기 위해 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit) 같은 변형 구조가 도입되었습니다.

---

- LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)의 추가 설명
    
    LSTM과 GRU는 RNN의 단점을 보완하기 위해 설계된 순환 신경망의 변형 구조로, **장기 의존성 문제**를 해결하는 데 초점을 맞춥니다.
    
    ### **LSTM(Long Short-Term Memory)**
    
    - **핵심 개념**: LSTM은 정보를 저장, 삭제, 업데이트하는 메커니즘이 포함된 셀 상태(cell state)와 여러 게이트를 사용합니다.
        - **셀 상태**: 장기적인 정보를 유지하거나 삭제할 수 있는 경로.
        - **게이트**: 셀 상태를 조작하는 세 가지 중요한 구성 요소:
            1. **입력 게이트(Input Gate)**: 새로운 정보를 셀 상태에 얼마나 추가할지 결정.
            2. **망각 게이트(Forget Gate)**: 이전 셀 상태의 정보를 얼마나 잊을지 결정.
            3. **출력 게이트(Output Gate)**: 다음 은닉 상태로 어떤 정보를 출력할지 결정.
    
    ### **GRU(Gated Recurrent Unit)**
    
    - **핵심 개념**: GRU는 LSTM에서 더 단순화된 구조를 가지고 있으며, 업데이트 게이트(Update Gate) 와 리셋 게이트(Reset Gate)라는 두 가지 게이트만 사용합니다.
        - **업데이트 게이트**: 정보를 얼마나 새롭게 유지할지 결정.
        - **리셋 게이트**: 이전 은닉 상태를 얼마나 무시할지 결정.
    - LSTM보다 계산이 효율적이고, 성능은 비슷하거나 더 나은 경우도 있음.

### **→ 책에서 케라스 API로 순환 신경망 구현 문제 해결 과정 요약**

### **설명**

이 절에서는 케라스를 활용해 순환 신경망을 구현하며, 두 가지 접근 방식을 실습:

1. **원-핫 인코딩을 사용한 순환층 입력 방식**
2. **단어 임베딩을 사용한 순환층 입력 방식**

### **핵심 내용**

- **데이터셋**: IMDB 리뷰 데이터셋.
    - 리뷰를 긍정/부정으로 분류하는 이진 분류 문제.
- **첫 번째 접근**:
    - **원-핫 인코딩**:
        - 텍스트 데이터를 정수로 변환한 후, 정수를 **어휘 사전 크기의 벡터**로 변환.
        - 단점: 메모리 소모가 크고 단어 간 관계를 표현하기 어려움.
- **두 번째 접근**:
    - **단어 임베딩(Embedding Layer)**:
        - 단어를 비교적 작은 크기의 **실수 밀집 벡터**로 변환.
        - 장점: 단어 간의 유사성이나 관계를 표현 가능.
        - 구현: 케라스의 **`Embedding`** 클래스를 사용해 임베딩 층 추가.

### **핵심 포인트**

1. **말뭉치**: 자연어 처리에서 사용하는 텍스트 데이터 모음.
2. **토큰**: 텍스트에서 공백으로 구분된 문자열.
3. **원-핫 인코딩**: 클래스별로 1개의 요소만 1이고 나머지는 0인 벡터.
4. **단어 임베딩**: 단어를 실수 벡터로 변환, 단어 간 관계를 잘 표현.
5. **케라스 주요 함수**:
    - **`pad_sequences()`**: 시퀀스 길이 맞추기(패딩 추가).
    - **`to_categorical()`**: 정수를 원-핫 인코딩으로 변환.
    - **`SimpleRNN`**: 기본 순환층 클래스, `tanh` 활성화 함수 사용.
    - **`Embedding`**: 단어 임베딩 구현을 위한 클래스.

---

## 3장

### **4 순환 신경망(RNN)과 LSTM/GRU 활용**

 기본 RNN, LSTM, GRU 구조를 이해하고 텐서플로와 케라스를 사용해 간단한 실습을 진행하며 감정 분류 문제를 해결한다.

- LSTM과 GRU 셀로 훈련 문제해결 과정에 대한 상세 설명
    
    **LSTM(Long Short-Term Memory) 셀**과 **GRU(Gated Recurrent Unit) 셀**은 순환 신경망에서 시간적으로 긴 데이터의 학습을 효과적으로 수행하기 위해 설계된 구조입니다. 각각의 작동 방식을 간단히 풀어보면:
    
    - **LSTM**:
        - LSTM은 **입력 게이트, 삭제 게이트, 출력 게이트**라는 세 가지 "문"으로 구성됩니다. 이 문들은 어떤 정보를 유지하거나 버릴지 결정합니다.
        - *셀 상태(cell state)**와 **은닉 상태(hidden state)**라는 두 가지 주요 정보를 다룹니다.
            - **셀 상태**는 오랜 시간 동안 기억해야 할 중요한 정보들을 저장하며, 다음 시점으로 전달됩니다.
            - **은닉 상태**는 매 타임스텝마다 결과를 출력하는 데 사용됩니다.
        - 이렇게 다양한 상태와 게이트를 통해, 긴 시퀀스의 데이터를 처리하면서도 중요한 정보를 잃지 않을 수 있습니다.
    - **GRU**:
        - GRU는 LSTM의 간소화 버전으로, **업데이트 게이트**와 **리셋 게이트**라는 두 가지 게이트만 사용합니다.
        - 이 간소화된 구조 덕분에 계산량이 줄어들어 LSTM보다 빠르게 작동하지만, 유사한 성능을 보일 때가 많습니다.
        - GRU는 **은닉 상태(hidden state)**만 사용하여 데이터를 처리합니다.
    
    **드롭아웃(Dropout)**:
    
    - 순환층에서 **드롭아웃**을 적용하면 과대적합(overfitting)을 방지할 수 있습니다. 드롭아웃은 학습 과정에서 일부 뉴런을 무작위로 제외하여 모델이 특정 패턴에 너무 의존하지 않도록 합니다.
    
    **테스트 과정**:
    
    1. 테스트 데이터는 훈련 데이터와 동일한 방식으로 전처리합니다. 이 예에서는 `pad_sequences()`를 사용하여 입력 데이터를 같은 길이로 맞췄습니다.
    2. 최적화된 모델을 저장한 파일(`best-2rnn-model.h5`)을 불러옵니다.
    3. `evaluate()` 메서드를 사용하여 테스트 데이터에서 모델 성능(손실 및 정확도)을 측정합니다.
    4. 결과는 테스트 데이터에서 80.19%의 정확도를 기록하여 성공적인 분류 모델임을 보여줍니다.

### **학습 내용**

1. **순환 신경망(RNN)의 기초**:
    - 순환 신경망이란?
    - 시퀀스 데이터 처리의 특징과 어려움(예: 장기 의존성 문제).
    - 단순 RNN의 작동 방식과 한계점.
2. **LSTM과 GRU의 차이**:
    - LSTM의 게이트 구조(입력, 삭제, 출력 게이트)와 셀 상태.
    - GRU의 간소화된 구조(업데이트 게이트, 리셋 게이트).
    - 각각의 장단점 및 활용 상황.
3. **케라스로 간단한 감정 분류 모델 만들기**:
    - **IMDB 데이터셋** 활용:
        - 긍정/부정 리뷰 분류 문제 해결.
    - 원-핫 인코딩 vs 단어 임베딩 비교:
        - `Embedding` 층 사용 방법.
    - 순환층의 구성:
        - `SimpleRNN`, `LSTM`, `GRU`를 사용한 모델 설계.
    - 드롭아웃과 다층 구성 방법.
4. **모델 평가 및 튜닝**:
    - `evaluate()` 메서드를 사용하여 테스트 데이터에서 성능 확인.
    - 과대적합 방지 전략(드롭아웃, 정규화 등).

---

### **핵심 포인트**

- **RNN의 한계**: 장기 의존성 문제로 인해 단순 RNN은 긴 시퀀스를 처리하기 어렵다.
- **LSTM/GRU**: 고급 게이트 메커니즘으로 장기 의존성 문제를 해결한다.
- **케라스 API 활용**:
    - `Embedding`으로 단어 임베딩 구현.
    - `LSTM`, `GRU`로 순환층 설계.
    - `Dropout`으로 과대적합 방지.

### **5 순환 신경망에서의 임베딩과 시퀀스 데이터 전처리**

---

### **학습 내용**

### 1. **단어 임베딩의 개념**

- **단어 임베딩(Word Embedding)**:
    - 단어를 고차원 공간의 실수 벡터로 표현.
    - 단어 간의 유사도를 벡터 간의 거리로 계산 가능.
    - 기존의 원-핫 인코딩과 비교하여 메모리 효율적이며, 문맥을 반영한 학습 가능.
- **임베딩의 학습 방식**:
    - 신경망 훈련 과정에서 임베딩 벡터가 자동으로 학습.
    
    ### Word2Vec, GloVe와 같은 사전 학습된 임베딩 활용 가능.
    
    ### 1. Word2Vec
    
    Word2Vec은 단어의 분포적 특징을 기반으로 단어를 고정된 크기의 벡터로 표현하는 알고리즘입니다.
    
    이 벡터는 단어 간의 의미적 유사성을 반영합니다.
    
    ### 주요 아이디어
    
    - **분포 가설**: "같은 문맥에서 자주 등장하는 단어는 의미적으로 유사하다."
    - 학습 방식:
        - **Skip-gram**: 주어진 단어로 주변 단어를 예측.
        - **CBOW (Continuous Bag of Words)**: 주변 단어로 주어진 단어를 예측.
    
    ### 특징
    
    - 단어를 유사한 의미의 벡터 공간으로 매핑.
    - 벡터 간의 연산이 가능 (예: King−Man+Woman≈Queen).
    - n-차원의 단어 벡터를 생성.
    
    ### 2. GloVe (Global Vectors for Word Representation)
    
    GloVe는 단어 간의 공존 행렬(co-occurrence matrix)을 사용하여 단어 벡터를 학습합니다.
    
    ### 주요 아이디어
    
    - 단어 간의 **공존 확률**을 기반으로 단어 벡터를 생성.
    - 단어의 의미적 관계를 희소 행렬에서 밀집 벡터로 변환.
    
    ### 특징
    
    - 문맥 정보뿐 아니라 단어의 **전역적 통계**를 반영.
    - Word2Vec보다 대규모 데이터에서 더 안정적.
- 공존행렬
    
    **공존 행렬**은 단어들 간의 **공존(co-occurrence)** 빈도를 기록한 행렬입니다. 텍스트 데이터에서 단어가 서로 가까운 위치에서 얼마나 자주 등장하는지를 나타내는 행렬로, 자연어 처리에서 단어 간 관계를 표현하는 기본적인 방식 중 하나입니다.
    

### 2. **시퀀스 데이터 전처리**

- **패딩(Padding)**:
    - 시퀀스 길이를 일정하게 맞추기 위해 부족한 길이를 0으로 채움.
    - `pad_sequences()` 함수 사용.
- **정규화 및 토큰화**:
    - 텍스트를 숫자로 변환.
    - 정규 표현식과 토큰화 라이브러리 활용.
- **단어 사전 크기 제한**:
    - 빈도가 낮은 단어를 제거하거나 "Out of Vocabulary(OOV)"로 처리.

### 3. **순환 신경망과 임베딩 결합**

- **Embedding 층 활용**:
    - 텍스트 데이터를 실수 벡터로 변환.
    - `Embedding(input_dim, output_dim)`:
        - `input_dim`: 단어 사전 크기.
        - `output_dim`: 임베딩 벡터의 차원.
- **순환층과의 결합**:
    - 임베딩 벡터를 입력으로 받아 순환층(LSTM, GRU)에서 처리.
    - 사전 학습된 임베딩을 로드하고 고정(frozen)하거나 추가 학습 가능.

### 4. **실습: 영화 리뷰 감정 분석**

- IMDB 데이터셋을 사용하여 긍정/부정 리뷰 분류.
- 단어 임베딩 적용 및 시퀀스 데이터 전처리 실습.

---

### **6 순환 신경망의 하이퍼파라미터 최적화와 성능 향상 전략**

### **학습 내용**

### 1. **하이퍼파라미터 최적화**

- **하이퍼파라미터의 종류**:
    - 학습률, 배치 크기, 드롭아웃 비율.
    - 순환층의 유닛 수와 층 수.
- **최적화 방법**:
    - **그리드 서치**:
        - 여러 조합의 하이퍼파라미터를 탐색.
        - 계산 비용이 높지만 체계적.
    - **랜덤 서치**:
        - 랜덤으로 조합을 선택해 효율적 탐색.
    - **베이지안 최적화**:
        - 이전 결과를 기반으로 다음 조합을 선택해 효율적.

### 2. **성능 향상 전략**

- **과대적합 방지**:
    - 드롭아웃: 과대적합을 억제하는 가장 일반적인 방법.
    - 조기 종료(Early Stopping): 검증 손실이 더 이상 감소하지 않을 때 학습 중단.
- **다중 순환층 사용**:
    - 모델의 표현력을 높이기 위해 순환층을 쌓음.
    - 각 층의 `return_sequences`를 True로 설정.
- **양방향 RNN**:
    - 순방향과 역방향 정보를 모두 학습.
    - `Bidirectional` 래퍼 사용.
- **사전 학습된 임베딩 활용**:
    - GloVe, Word2Vec 임베딩을 사용하여 빠른 수렴 유도.

### 3. **실습: 하이퍼파라미터 튜닝**

- IMDB 데이터셋에서 모델의 성능을 최적화.

### **핵심 포인트 요약**

- 하이퍼파라미터 최적화는 모델의 성능 향상에 핵심적인 역할을 한다.
- 과대적합 방지와 다중 순환층, 양방향 RNN 사용은 성능 향상에 효과적이다.
- 사전 학습된 임베딩은 텍스트 데이터의 문맥 학습을 가속화한다.

---

### **7 SLAM과 LLM**

### **학습 내용**

### 1. **SLAM과 텍스트 기반 데이터**

- **SLAM의 개념**:
    - 로봇이 미지의 환경에서 현재 위치를 추정하며 지도를 작성하는 알고리즘.
    - 텍스트 데이터를 기반으로 가상의 "지도" 작성 가능.
- **텍스트 기반 SLAM**:
    - 문서 내에서 주제와 키워드 간의 관계를 "지도"로 시각화.
    - LLM을 사용하여 주요 주제와 하위 키워드 추출.

### 2. **LLM과 SLAM의 결합**

- **LLM 활용**:
    - 텍스트 데이터를 처리하고 문맥을 이해하며 주요 정보를 추출.
    - RAG(Retrieval-Augmented Generation)로 확장 가능.
- **지도 작성 과정**:
    - 순환 신경망과 임베딩을 사용해 텍스트 관계를 벡터 공간에 매핑.
    - SLAM 알고리즘과 결합하여 주제 간의 유사성을 지도 형태로 시각화.

---

### **핵심 포인트**

- SLAM 개념을 텍스트 분석에 적용하여 주제 간의 관계를 "지도"로 표현할 수 있다.
- LLM은 문맥을 이해하고 중요한 정보를 추출하는 데 강력한 도구로 활용된다.
- 텍스트 데이터를 벡터 공간으로 매핑한 후, 차원 축소 및 시각화 기법으로 분석 가능하다.
- 시각화의 장점
    
    텍스트 만드는 과정을 시각화하는 것은 데이터의 **구조, 관계, 주요 특징**을 이해하고 분석하는 데 유용
    

버토픽도 이 주제들과 연관이 있었다!

### **토픽 간 관계 시각화**

- **UMAP**이나 **t-SNE**를 통해 고차원 데이터를 저차원 공간으로 축소하여 문서나 단어 간 관계를 점으로 표현.
- 각 토픽이 어떻게 연결되어 있는지 확인 가능.

---

### **9-1: 순차 데이터와 순환 신경망**

### **텍스트 데이터의 특성과 전처리**

1. **순차 데이터 (Sequential Data)**
    - 순서에 의미가 있는 데이터.
    - 예시: 글, 대화, 일자별 날씨, 판매 실적 등.
2. **순환 신경망 (Recurrent Neural Network, RNN)**
    - 순차 데이터를 처리하기에 적합한 인공 신경망의 한 종류.
    - 순환층(Recurrent Layer)을 1개 이상 포함한 신경망.
3. **셀 (Cell)**
    - 순환층의 기본 구성 요소.
    - 여러 개의 뉴런으로 구성됨.
4. **은닉 상태 (Hidden State)**
    - 셀의 출력 값.
    - 다음 층으로 전달되며, 셀이 다음 타임스텝의 데이터를 처리할 때 재사용됨.

→ 엇 잘 모르겠다.

### **은닉 상태의 추가 설명**

은닉 상태가 다음 층으로 전달되고 재사용된다는 부분은 **RNN의 구조적 특징**과 밀접하게 관련되어 있습니다. 이를 이해하려면 은닉 상태의 역할과 순차 데이터 처리 방식에 대해 조금 더 자세히 살펴볼 필요가 있습니다.

1. **은닉 상태란 무엇인가?**
    - RNN에서 각 타임스텝(time step)은 입력 데이터를 처리합니다.
    - 은닉 상태(Hidden State)는 **이전 타임스텝에서의 중요한 정보를 요약한 값**으로, 현재 타임스텝의 입력과 함께 다음 출력을 계산하는 데 사용됩니다.
    - 쉽게 말해, 은닉 상태는 **입력 데이터의 맥락(Context)을 기억하고 유지**하는 역할을 합니다.
2. **다음 층으로 전달되는 이유**
    - RNN은 **순차 데이터의 시간적 의존성**을 처리하기 위해 설계되었습니다.
    - 은닉 상태를 다음 층으로 전달함으로써 **과거 정보가 현재와 미래의 계산에 반영**될 수 있습니다.
    - 예를 들어, 텍스트 데이터에서 "I like"라는 구문이 있다면, "I"의 정보는 "like"를 처리하는 데 필요합니다. 이 정보를 은닉 상태가 전달합니다.
3. **타임스텝에서 재사용되는 이유**
    - 은닉 상태는 **각 타임스텝에서의 중간 결과물**이므로, 이후 타임스텝에서 새로운 입력 데이터를 처리할 때 **기존의 맥락과 결합**하여 업데이트됩니다.
    - 이 과정은 **순환(Recurrent)** 구조의 본질을 이루며, 데이터를 시간 축에 따라 단계적으로 처리하게 됩니다.
4. **작동 방식 (수식으로 표현)**

### **바이어스에 대한 추가 설명**

- 신경망에서 각 뉴런의 출력에 일정한 상수를 더해주는 값입니다.
- 주어진 데이터 패턴에 맞춰 가중치와 함께 모델의 학습을 돕는 역할을 합니다.
- 이를 통해 데이터의 **비선형성**을 더 잘 모델링할 수 있습니다.
- 오차와는 다른 값, 오차를 줄이기 위해서 가중치와 함께 이용된다.

---

### **9-2: 순환 신경망으로 IMDB 리뷰 분류하기**

### **1. IMDB 리뷰 데이터셋 개요**

- IMDB 데이터셋은 영화 리뷰 텍스트를 기반으로 **긍정(Positive)** 또는 **부정(Negative)**으로 분류하는 이진 분류(Binary Classification) 문제를 다룹니다.
- 적재하기 : IMDB 리뷰 데이터는 정수로 변환

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/e92bed44-5976-44f3-9a73-1b92d4d57f3f/ceec2b8b-bcf4-4066-a902-060c60f7cc19/image.png)

---

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/e92bed44-5976-44f3-9a73-1b92d4d57f3f/f498feba-5fc9-4da4-8089-890c0f3995a3/image.png)

주의! IMDB 리뷰 텍스트는 길이가 제각각입니다. 따라서 고정 크기의 2차원 배열에 담기 보다는 리뷰마다 별도의 파이썬 리스트로 담아야 메모리를 효율적으로 사용할 수 있습니다.

### **2. 단어 임베딩(Embedding Layer):**

해결할 문제는 리뷰가 긍정인지 부정인지를 판단. 그러면 이진 분류 문제로 볼 수 있으므로 타깃값이 0（부정）과 1 （긍정）로 나누어집니다.

### **기존 원-핫 인코딩의 한계**

- 단어를 차원 벡터로 표현하지만 단어 간의 관계나 의미를 반영하지 못함.
- 매우 높은 차원의 벡터를 사용하므로 메모리 비효율적.

### **임베딩 레이어의 특징**

1. **효율적 표현**:
    - 단어를 밀집 벡터(Dense Vector)로 변환해, 저차원에서 단어 간의 의미를 학습.
2. **문맥 정보 학습**:
    - 단어의 의미적 유사성을 벡터 간의 거리로 나타냄.
    - 학습 중 데이터의 문맥을 반영하여 벡터를 최적화.
3. **사용 방법**:
    - 케라스 `Embedding` 레이어를 활용.
    - 사전 학습된 임베딩(예: GloVe, Word2Vec)을 로드하거나 임베딩 가중치를 학습 가능.

---

### **3. 케라스 API를 활용한 RNN 모델 구현**

### **전처리: 시퀀스 길이 맞추기**

1. **`pad_sequences()`**:
    - 입력 데이터는 서로 다른 길이의 단어 시퀀스로 이루어져 있음.
    - 모델 학습을 위해 모든 입력의 길이를 동일하게 맞춰야 함.
    - 짧은 시퀀스는 0으로 패딩(padding), 긴 시퀀스는 잘라냄(truncating).
    
    ```python
    from keras.preprocessing.sequence import pad_sequences
    
    # 패딩 예제
    max_length = 100
    X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')
    X_test = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')
    
    ```
    

### **RNN 모델 구현**

1. **모델 구조**:
    - 임베딩 레이어: 입력 단어를 벡터로 변환.
    - RNN 레이어(`SimpleRNN`): 시퀀스 데이터를 처리하여 시계열 정보를 학습.
    - Dense 레이어: 최종 출력을 생성.
2. **구현 예제**:
    
    ```python
    from keras.models import Sequential
    from keras.layers import Embedding, SimpleRNN, Dense
    
    model = Sequential([
        Embedding(input_dim=20000, output_dim=128, input_length=100),  # 임베딩 레이어
        SimpleRNN(64, return_sequences=False),                         # RNN 레이어
        Dense(1, activation='sigmoid')                                # 출력 레이어
    ])
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    ```
    

---

### **4. 모델 훈련 및 테스트**

### **훈련**

1. 학습 데이터를 모델에 입력.
2. 조기 종료(Early Stopping)와 모델 저장(Checkpoint)을 사용해 최적의 가중치만 저장.
    
    ```python
    from keras.callbacks import ModelCheckpoint, EarlyStopping
    
    checkpoint = ModelCheckpoint('best-2rnn-model.h5', save_best_only=True)
    early_stopping = EarlyStopping(patience=3, restore_best_weights=True)
    
    model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32,
              callbacks=[checkpoint, early_stopping])
    
    ```
    

### **테스트**

1. 저장된 모델 불러오기:
    
    ```python
    from keras.models import load_model
    
    model = load_model('best-2rnn-model.h5')
    
    ```
    
2. 테스트 데이터 평가:
    
    ```python
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Loss: {loss}")
    print(f"Test Accuracy: {accuracy}")
    
    ```
    

---

### **9-3: LSTM과 GRU 셀**

### **LSTM(Long Short-Term Memory)**

1. **LSTM의 주요 구성 요소**
    - 입력 게이트(Input Gate), 망각 게이트(Forget Gate), 출력 게이트(Output Gate).
    - 셀 상태(cell state)와 은닉 상태(hidden state)의 역할.
2. **GRU(Gated Recurrent Unit)**
    - 업데이트 게이트와 리셋 게이트로 구성.
    - LSTM에 비해 계산 효율성이 높고, 유사한 성능을 제공.

### **드롭아웃(Dropout)**

- 과대적합 방지를 위해 일부 뉴런을 무작위로 제외.

### **다층 순환 신경망**

- 여러 순환층을 쌓아 모델의 표현력을 향상.
